{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b21d840-2b0f-4d8c-afce-063e71acbe18",
   "metadata": {},
   "source": [
    "###### The BM25 algorithm is designed to rank documents based on the relevance of terms in a query, considering factors like term frequency and document length. However, if your query doesn't match any documents exactly, you might need to adjust your approach to improve the similarity detection. Here are some strategies to enhance the effectiveness of BM25 in finding more relevant matches:\n",
    "######  Implemnted in this new script\n",
    "###### Synonyms and Stemming: Use techniques like stemming or lemmatization to reduce words to their base forms, and consider expanding your query with synonyms to capture more variations of the terms. \n",
    "###### Query Expansion: Manually or automatically expand your query with related terms. This can be done using a thesaurus or word embeddings like Word2Vec or GloVe to find semantically similar words.\n",
    "###### Preprocessing Enhancements: Improve your preprocessing steps by removing noise, handling typos, and ensuring consistent formatting across your dataset.\n",
    "###### Custom Scoring: Consider implementing a custom scoring function that combines BM25 with other metrics, such as semantic similarity using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2138a96c-e16b-4b1a-87ab-829dad563560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oscarahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365    <p></p><p><b><span style=\"font-size: 14px;\">&n...\n",
      "291    <p>link_mca_ctl.mask_ecc_error , when set to 1...\n",
      "114    <p class=\"MsoNormal\">&nbsp;<span style=\"font-s...\n",
      "128    <p style=\"margin:0in\"><span style=\"font-size: ...\n",
      "123    <p style=\"margin:0in;font-family:Calibri;font-...\n",
      "Name: description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'C:/Users/oscarahe/OneDrive - Intel Corporation/Desktop/Exceles/query2.csv'  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select the column to compare against\n",
    "column_to_compare = 'description'  # Replace with your column name\n",
    "\n",
    "# Initialize stemmer. This helps to reduce words to their base forms. New feature\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Tokenize, stem, and remove stop words\n",
    "    tokens = [stemmer.stem(word) for word in text.split() if word not in ENGLISH_STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "# Expand query with synonyms. This is a new feature. \n",
    "def expand_query(query):\n",
    "    expanded_query = set(query)\n",
    "    for word in query:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_query.add(stemmer.stem(lemma.name()))\n",
    "    return list(expanded_query)\n",
    "\n",
    "# Preprocess the data in the selected column\n",
    "documents = df[column_to_compare].apply(preprocess_text).tolist()\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(documents)\n",
    "\n",
    "# Statement to compare\n",
    "statement = \"BIOS\"  # Replace with your statement\n",
    "query = preprocess_text(statement)\n",
    "\n",
    "# Expand the query\n",
    "expanded_query = expand_query(query)\n",
    "\n",
    "# Get BM25 scores\n",
    "scores = bm25.get_scores(expanded_query)\n",
    "\n",
    "# Find the indices of the top 5 scores\n",
    "top_n = 5\n",
    "top_indices = scores.argsort()[-top_n:][::-1]\n",
    "\n",
    "# Get the top 5 most similar sightings\n",
    "top_sightings = df.iloc[top_indices]\n",
    "\n",
    "#print(\"Top 5 most similar sightings:\")\n",
    "print(top_sightings[column_to_compare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656271ee-f2fa-4634-ab26-7550962149c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\oscarahe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar sightings:\n",
      "114    <p class=\"MsoNormal\">&nbsp;<span style=\"font-s...\n",
      "365    <p></p><p><b><span style=\"font-size: 14px;\">&n...\n",
      "286    <p>During early BIOS boots we're seeing interm...\n",
      "227    <br /><p>Tested on EMR A0 IFWI 94D13 and BIOS ...\n",
      "257    <p><span style=\"font-weight: bolder;\"></span><...\n",
      "Name: description, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"Remove HTML tags from a string.\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by removing HTML, converting to lowercase, tokenizing, stemming, and removing stop words.\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = remove_html(text)\n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    # Tokenize, stem, and remove stop words\n",
    "    tokens = [stemmer.stem(word) for word in text.split() if word not in ENGLISH_STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "def expand_query(query):\n",
    "    \"\"\"Expand query with synonyms.\"\"\"\n",
    "    expanded_query = set(query)\n",
    "    for word in query:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_query.add(stemmer.stem(lemma.name()))\n",
    "    return list(expanded_query)\n",
    "\n",
    "def load_data(file_path, column_name):\n",
    "    \"\"\"Load data from a CSV file and preprocess the specified column.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        documents = df[column_name].apply(preprocess_text).tolist()\n",
    "        return df, documents\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return None, None\n",
    "\n",
    "def get_top_sightings(df, documents, statement, top_n=5):\n",
    "    \"\"\"Retrieve top N most similar sightings based on BM25 scores.\"\"\"\n",
    "    query = preprocess_text(statement)\n",
    "    expanded_query = expand_query(query)\n",
    "    bm25 = BM25Okapi(documents)\n",
    "    scores = bm25.get_scores(expanded_query)\n",
    "    top_indices = scores.argsort()[-top_n:][::-1]\n",
    "    return df.iloc[top_indices]\n",
    "\n",
    "def main(file_path, column_name, statement):\n",
    "    \"\"\"Main function to execute the retrieval process.\"\"\"\n",
    "    df, documents = load_data(file_path, column_name)\n",
    "    if df is not None:\n",
    "        top_sightings = get_top_sightings(df, documents, statement)\n",
    "        print(\"Top 5 most similar sightings:\")\n",
    "        print(top_sightings[column_name])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'C:/Users/oscarahe/OneDrive - Intel Corporation/Desktop/Exceles/query2.csv'\n",
    "    column_name = 'description'\n",
    "    statement = \"BIOS\"\n",
    "    main(file_path, column_name, statement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422a31d5-44eb-4a71-9835-510b1bdb0dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observing completely closed eye once xtalk is enabled and not able to reach 0 BER showing fast unit bathtub plot as a reference, fast unit eye is small but able to reach 0 BER and perform JTOL  <img\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_malformed(text):\n",
    "    \"\"\"Remove HTML tags from a malformed HTML string using BeautifulSoup.\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Example usage with malformed HTML\n",
    "malformed_html_content = \"<div>Observing completely closed eye once xtalk is enabled and not able to reach 0 BER showing fast unit bathtub plot as a reference, fast unit eye is small but able to reach 0 BER and perform JTOL&nbsp;</div><div><img src=https://hsdes.intel.com/rest/binary/14019102588 data-filename=image.png style=width: 531px; />&nbsp;<img\"\n",
    "clean_text = remove_html_malformed(malformed_html_content)\n",
    "print(clean_text)  # Output: This is a sample text with HTML tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
